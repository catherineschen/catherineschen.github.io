---
title:          "Evaluating Search System Explainability with Psychometrics and Crowdsourcing"
date:           2024-07-11 00:00:00 +0800
selected:       true
pub:            "SIGIR"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
pub_date:       "2024"

abstract: >-
  Information retrieval (IR) systems have become an integral part of our everyday lives. As search engines, recommender systems, and conversational agents are employed across various domains from recreational search to clinical decision support, there is an increasing need for transparent and explainable systems to guarantee accountable, fair, and unbiased results. Despite many recent advances towards explainable AI and IR techniques, there is no consensus on what it means for a system to be explainable. Although a growing body of literature suggests that explainability is comprised of multiple subfactors, virtually all existing approaches treat it as a singular notion. In this paper, we examine explainability in Web search systems, leveraging psychometrics and crowdsourcing to identify human-centered factors of explainability.
# cover:          /assets/images/covers/cover3.jpg
authors:
  - Catherine Chen
  - Carsten Eickhoff
links:
  Paper: https://arxiv.org/abs/2210.09430
  Code: https://github.com/catherineschen/sse-metric
---
